# logistic regression

[Logistic Regression From Scratch in Python](https://medium.com/p/ec66603592e2)

[Machine-learning-by-Andrew-NG--Python-/LogisticRegression.ipynb at master · IraAI/Machine-learning-by-Andrew-NG--Python-](https://github.com/IraAI/Machine-learning-by-Andrew-NG--Python-/blob/master/LogisticRegression.ipynb)

[Derivative of Log-Loss function for Logistic Regression](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d)

- setting
    
    m: # of training example, n# number of features 
    

 sigmoid function g(z):  the probability of predicting x to be 1 

$$
h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^{T}x}}=P(y=1;x,\theta)\\ g(z)=\frac{1}{1+e^{-z}}
$$

- loss function: intuition
    
    ![1921662877253_.pic.jpg](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/1921662877253_.pic.jpg)
    
    ![1931662877262_.pic.jpg](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/1931662877262_.pic.jpg)
    
- simplified loss function:
    
    ![Untitled](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/Untitled.png)
    

- derivation:
    
    step1:
    
    ![Untitled](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/Untitled%201.png)
    
    step2:
    
    ![Untitled](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/Untitled%202.png)
    
    step3:
    
    ![Untitled](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/Untitled%203.png)
    
    ![1941662877682_.pic.jpg](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/1941662877682_.pic.jpg)
    
- code
    
    [Logistic-Regression-From-Scratch-Python/LogisticRegressionImplementation.ipynb at master · aihubprojects/Logistic-Regression-From-Scratch-Python](https://github.com/aihubprojects/Logistic-Regression-From-Scratch-Python/blob/master/LogisticRegressionImplementation.ipynb)
    
    ```python
    class LogisticRegression:
    
    # defining parameters such as learning rate, number ot iterations, whether to include intercept, 
        # and verbose which says whether to print anything or not like, loss etc.
        def __init__(self, learning_rate=0.01, num_iterations=50000, fit_intercept=True, verbose=False):
            self.learning_rate = learning_rate
            self.num_iterations = num_iterations
            self.fit_intercept = fit_intercept
            self.verbose = verbose
    		def __b_intercept(self, X):
            # initially we set it as all 1's
            intercept = np.ones((X.shape[0], 1))
            #then we concatinate them to the value of X, we don't add we just append them at the end.
            return np.concatenate((intercept, X), axis=1)
    
        def __sigmoid_function(self, z):
            # this is our actual sigmoid function which predicts our yp
            return 1 / (1 + np.exp(-z))
    
        def __loss(self, yp, y):
            # this is the loss function which we use to minimize the error of our model
            return (-y * np.log(yp) - (1 - y) * np.log(1 - yp)).mean()
    
         # this is the function which trains our model.
        def fit(self, X, y):
            
            # as said if we want our intercept term to be added we use fit_intercept=True
            if self.fit_intercept:
                X = self.__b_intercept(X)
            
            # weights initialization of our Normal Vector, initially we set it to 0, then we learn it eventually
            self.W = np.zeros(X.shape[1])
            
            # this for loop runs for the number of iterations provided
            for i in range(self.num_iterations):
                
                # this is our W * Xi
                z = np.dot(X, self.W)
                
                # this is where we predict the values of Y based on W and Xi
                yp = self.__sigmoid_function(z)
                
                # this is where the gradient is calculated form the error generated by our model
                gradient = np.dot(X.T, (yp - y)) / y.size
                
                # this is where we update our values of W, so that we can use the new values for the next iteration
                self.W -= self.learning_rate * gradient
                
                # this is our new W * Xi
                z = np.dot(X, self.W)
                yp = self.__sigmoid_function(z)
                
                # this is where the loss is calculated
                loss = self.__loss(yp, y)
                
                # as mentioned above if we want to print somehting we use verbose, so if verbose=True then our loss get printed
                if(self.verbose ==True and i % 10000 == 0):
                    print(f'loss: {loss} \t')
    
            # this is where we predict the probability values based on out generated W values out of all those iterations.
         def predict_prob(self, X):
            # as said if we want our intercept term to be added we use fit_intercept=True
            if self.fit_intercept:
                X = self.__b_intercept(X)
            
            # this is the final prediction that is generated based on the values learned.
            return self.__sigmoid_function(np.dot(X, self.W))
        
        # this is where we predict the actual values 0 or 1 using round. anything less than 0.5 = 0 or more than 0.5 is 1
          def predict(self, X):
    
    ```
    

```python
#Let us try creating a class of it, we will give Learning rate as 0.1 
#and number of iterations as 300000
model = LogisticRegression(learning_rate=0.1, num_iterations=300000)
model.fit(X, y)
preds = model.predict(X)
(preds == y).mean()

plt.figure(figsize=(10, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')
plt.legend()
x1_min, x1_max = X[:,0].min(), X[:,0].max(),
x2_min, x2_max = X[:,1].min(), X[:,1].max(),
xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))
grid = np.c_[xx1.ravel(), xx2.ravel()]
probs = model.predict_prob(grid).reshape(xx1.shape)
plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black');
```

![Untitled](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/Untitled%204.png)

sklearn

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
model = LogisticRegression(solver='liblinear', random_state=0)

model.fit(X, y)
#LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#                   intercept_scaling=1, l1_ratio=None, max_iter=100,
 #                  multi_class='auto', n_jobs=None, penalty='l2',
#                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,
 #                  warm_start=False)

model.predict_proba(X)

cm = confusion_matrix(y, model.predict(X))

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='white')
plt.show()

print(classification_report(y, model.predict(X)))
```

![Untitled](logistic%20regression%2083a94c53a57844efbcda7d8ae3efaaca/Untitled%205.png)