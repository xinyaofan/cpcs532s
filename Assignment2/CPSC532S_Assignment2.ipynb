{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the dataset provided as a separate link on the course webpage and extract it into `data/`. The dataset contains approximately 20K training images and 100 validation images, with multiple captions/tags for each image. For this assignment, we are only concerned with the tags and ignore the captions.\n",
    "\n",
    "For question two on the assignment, the dataset also contains a JSON file that maps from the ImageNet labels to the category names. \n",
    "\n",
    "Following the data downloading and unzipping, the code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0039, 0.0078, 0.0039,  ..., 0.0471, 0.0471, 0.0314],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0353, 0.0353, 0.0392],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0392, 0.0392, 0.0510],\n",
       "          ...,\n",
       "          [0.7137, 0.7294, 0.7137,  ..., 0.1686, 0.1843, 0.1686],\n",
       "          [0.7059, 0.6902, 0.6863,  ..., 0.1765, 0.1804, 0.2039],\n",
       "          [0.6784, 0.6667, 0.6706,  ..., 0.1922, 0.2157, 0.2275]],\n",
       "\n",
       "         [[0.1490, 0.1490, 0.1412,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.1451, 0.1412, 0.1373,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.1412, 0.1373, 0.1373,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.4392, 0.4667, 0.4549,  ..., 0.2588, 0.2745, 0.2863],\n",
       "          [0.4353, 0.4235, 0.4196,  ..., 0.2745, 0.2980, 0.3137],\n",
       "          [0.4118, 0.4000, 0.4000,  ..., 0.3020, 0.3176, 0.3020]],\n",
       "\n",
       "         [[0.5294, 0.5294, 0.5294,  ..., 0.1451, 0.1412, 0.1333],\n",
       "          [0.5255, 0.5333, 0.5373,  ..., 0.1725, 0.1451, 0.1412],\n",
       "          [0.5373, 0.5490, 0.5451,  ..., 0.2314, 0.1843, 0.1608],\n",
       "          ...,\n",
       "          [0.0118, 0.0078, 0.0078,  ..., 0.5216, 0.5294, 0.5137],\n",
       "          [0.0078, 0.0078, 0.0118,  ..., 0.5098, 0.5216, 0.5216],\n",
       "          [0.0078, 0.0118, 0.0039,  ..., 0.5294, 0.5255, 0.4784]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor).unsqueeze(0)\n",
    "    # return image_var.cuda()\n",
    "    return image_var\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load ImageNet label to category name mapping.\n",
    "imagenet_categories = [value for key,value in sorted(json.load(open('data/imagenet_categories.json')).items(), key=lambda t: int(t[0]))]\n",
    "\n",
    "# Load annotations file for the 20K training images.\n",
    "mscoco_train = json.load(open('data/annotations/train2014.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "category_to_name = {entry['id']: entry['name'] for entry in mscoco_train['categories']}\n",
    "category_idx_to_name = [entry['name'] for entry in mscoco_train['categories']]\n",
    "category_to_idx = {entry['id']: i for i,entry in enumerate(mscoco_train['categories'])}\n",
    "\n",
    "# Load annotations file for the 100 validation images.\n",
    "mscoco_val = json.load(open('data/annotations/val2014.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# We extract out all of the category labels for the images in the training set. We use a set to ignore \n",
    "# duplicate labels.\n",
    "train_id_to_categories = defaultdict(set)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    train_id_to_categories[entry['image_id']].add(entry['category_id'])\n",
    "\n",
    "# We extract out all of the category labels for the images in the validation set. We use a set to ignore \n",
    "# duplicate labels.\n",
    "val_id_to_categories = defaultdict(set)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    val_id_to_categories[entry['image_id']].add(entry['category_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at an image and its corresponding category labels. We consider the image with the id 391895 and the corresponding filename, `data/val2014/COCO_val2014_000000391895.jpg`. The image is shown below.\n",
    "\n",
    "![image](data/val2014/COCO_val2014_000000391895.jpg)\n",
    "\n",
    "The following code determines the category labels for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,category in enumerate(val_id_to_categories[391895]):\n",
    "    print(\"%d. %s\" % (i, category_to_name[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a Pre-trained Convolutional Neural Network (CNN)\n",
    "\n",
    "We will work with the VGG-16 image classification CNN network first introduced in [Very Deep Convolutional Neural Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf) by K. Simonyan and A. Zisserman.\n",
    "\n",
    "Fairly straightforwardly, we load the pre-trained VGG model and indicate to PyTorch that we are using the model for inference rather than training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_model = models.vgg16(pretrained=True).cuda()\n",
    "vgg_model.eval()\n",
    "\n",
    "# Let's see what the model looks like.\n",
    "vgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making Predictions Using VGG-16\n",
    "\n",
    "Given the pre-trained network, we must now write the code to make predictions on the 10 validation images via a forward pass through the network. Typically the final layer of VGG-16 is a softmax layer, however the pre-trained PyTorch model that we are using does not have softmax built into the final layer (instead opting to incorporate it into the loss function) and therefore we must **manually** apply softmax to the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()\n",
    "for image_id in val_ids[:10]:\n",
    "    # Display the image.\n",
    "    # -- Your code goes here --\n",
    "\n",
    "    # Print all of the category labels for this image.\n",
    "    # -- Your code goes here --\n",
    "  \n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(val_id_to_file[image_id])\n",
    "\n",
    "    # Run the image through the model and softmax.\n",
    "    label_likelihoods = softmax(vgg_model(img)).squeeze()\n",
    "\n",
    "    # Get the top 5 labels, and their corresponding likelihoods.\n",
    "    probs, indices = label_likelihoods.topk(5)\n",
    "\n",
    "    # Iterate and print out the predictions.\n",
    "    # -- Your code goes here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing Generic Visual Features using CNN\n",
    "\n",
    "Since, rather than the output of VGG, we want a fixed sized vector representation of each image, we remove the last linear layer. The implementation of the forward function for VGG is shown below:\n",
    "\n",
    "```\n",
    "x = self.features(x)\n",
    "x = x.view(x.size(0), -1)\n",
    "x = self.classifier(x)\n",
    "```\n",
    "We aim to preserve everything but the final component of the classifier, meaning we must define an alternative equivalent to `self.classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the final layer of the classifier, and indicate to PyTorch that the model is being used for inference\n",
    "# rather than training (most importantly, this disables dropout).\n",
    "    \n",
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we vectorize all of the features of training images and write the results to a file.\n",
    "\n",
    "# -- Your code goes here --\n",
    "    \n",
    "np.save(open('outputs/training_vectors', 'wb+'), training_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next we vectorize all of the features of validation images and write the results to a file.\n",
    "    \n",
    "# -- Your code goes here --\n",
    "    \n",
    "np.save(open('outputs/validation_vectors', 'wb+'), validation_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visual Similarity\n",
    "\n",
    "We now use the generated vectors, to find the closest training image for each validation image. This can easily be done by finding the training vector that minimizes the Euclidean distance for every validation image. We repeat this exercise for the first 10 images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training a Multi-Label Classification Network\n",
    "\n",
    "We now build a two layer classification network, which takes 4096-dimensional vectors as input and outputs the probabilities of the 80 categories present in MSCOCO. \n",
    "\n",
    "For this purpose, we utilize two layers (both containing sigmoid activation functions) with the hidden dimension set to 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we construct a class for the model\n",
    "# -- Your code goes here --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The output data is prepared by representing each output as a binary vector of categories\n",
    "# -- Your code goes here --\n",
    "\n",
    "def train(model, learning_rate=0.001, batch_size=100, epochs=5):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # -- Your code goes here --\n",
    "\n",
    "# Finally train the model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now repeat step two using the two layer classifier.\n",
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. End-to-End Model Fine-tuning\n",
    "\n",
    "Instead of training *only* the final two layers, we now create an end-to-end model and train the entire thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we construct a class for the model\n",
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The output data is prepared by representing each output as a binary vector of categories\n",
    "# -- Your code goes here --\n",
    "\n",
    "def train(model, learning_rate=0.001, batch_size=50, epochs=2):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # -- Your code goes here --\n",
    "    \n",
    "# Finally train the model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now repeat step two using the end-to-end classifier.\n",
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyper-parameter Tuning\n",
    "\n",
    "Now we do a grid search over the learning rate and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Your code goes here --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
