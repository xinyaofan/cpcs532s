{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnzrOsd0bZHS"
      },
      "source": [
        "# CPSC532S Assignment 3:  RNNs for Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "t_cnRUY7dkdI"
      },
      "source": [
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from gensim.models import Word2Vec\n",
        "from random import random\n",
        "from nltk import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oSmx8s7dkdK"
      },
      "source": [
        "# Data Acquisition\n",
        "\n",
        "\n",
        "The goal of this assignment is to translate English to Pig Latin. For this assignment, you must download the data and extract it into `data/`. The dataset contains four files, each containing a single caption on each line. There are two files for training (English vs Pig Latin) and two files for validation. We should have 20,000 sentences (one sentence per image in Assignment 2) in the training captions and 500 sentences in the validation captions (five sentences per image in Assignment 2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS3DYtiqdkdL",
        "outputId": "00f46302-f9fa-4896-b7b5-ea5841f37ad6"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the data into memory.\n",
        "mscoco_train = json.load(open(\"/content/drive/My Drive/Colab Notebooks/train_captions.json\"))\n",
        "mscoco_val  = json.load(open('/content/drive/My Drive/Colab Notebooks/val_captions.json'))\n",
        "\n",
        "mscoco_piglatin_train = json.load(open('/content/drive/My Drive/Colab Notebooks/piglatin_train_captions.json'))\n",
        "mscoco_piglatin_val  = json.load(open('/content/drive/My Drive/Colab Notebooks/piglatin_val_captions.json'))\n",
        "\n",
        "train_sentences = [entry['caption'] for entry in mscoco_train['annotations']]\n",
        "val_sentences = [entry['caption'] for entry in mscoco_val['annotations']]\n",
        "\n",
        "piglatin_train_sentences = [entry['caption'] for entry in mscoco_piglatin_train['annotations']]\n",
        "piglatin_val_sentences = [entry['caption'] for entry in mscoco_piglatin_val['annotations']]\n",
        "\n",
        "print(len(train_sentences))\n",
        "print(len(val_sentences))\n",
        "print(len(piglatin_train_sentences))\n",
        "print(len(piglatin_val_sentences))\n",
        "print(train_sentences[0])\n",
        "print(piglatin_train_sentences[0])\n",
        "print(val_sentences[0])\n",
        "print(piglatin_val_sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "20000\n",
            "500\n",
            "20000\n",
            "500\n",
            "A very clean and well decorated empty bathroom\n",
            "Away eryvay eanclay andway ellway ecoratedday emptyway athroombay\n",
            "Set of bananas hanging off of a banana tree.\n",
            "Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyGm8VmCdkdM"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYWipM4tdkdN",
        "outputId": "9104af99-d1cc-4ead-f9cb-5b75d6f34e17"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = train_sentences\n",
        "piglatin_sentences = piglatin_train_sentences\n",
        "\n",
        "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
        "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
        "piglatin_sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in piglatin_sentences]\n",
        "\n",
        "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
        "vocabularySize = 2000\n",
        "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
        "piglatin_word_counts = Counter([word for sentence in piglatin_sentences for word in sentence])\n",
        "word_counts = word_counts + piglatin_word_counts\n",
        "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
        "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
        "\n",
        "# Build the one hot embeddings\n",
        "one_hot_embeddings = np.eye(vocabularySize)\n",
        "\n",
        "\n",
        "# Build the word2vec embeddings\n",
        "wordEncodingSize = 300\n",
        "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
        "piglatin_filtered_sentences = [[word for word in sentence if word in word2index] for sentence in piglatin_sentences]\n",
        "all_filtered_sentences = filtered_sentences + piglatin_filtered_sentences\n",
        "w2v = Word2Vec(all_filtered_sentences, min_count=0, size=wordEncodingSize)\n",
        "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
        "\n",
        "# Define the max sequence length to be the longest sentence in the training data. \n",
        "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
        "piglatin_maxSequenceLength = max([len(sentence) for sentence in piglatin_sentences])\n",
        "\n",
        "if piglatin_maxSequenceLength > maxSequenceLength:\n",
        "    maxSequenceLength = piglatin_maxSequenceLength\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zYnaPC0i0A2"
      },
      "source": [
        "# Utilities functions\n",
        "\n",
        "\n",
        "Please look through the functions provided below carefully, as you will need to use all of them at some point in your assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PFOm2o3jINX",
        "outputId": "a323b76d-94a2-4465-ea0b-7a37b00189be"
      },
      "source": [
        "def preprocess_numberize(sentence):\n",
        "    \"\"\"\n",
        "    Given a sentence, in the form of a string, this function will preprocess it\n",
        "    into list of numbers (denoting the index into the vocabulary).\n",
        "    \"\"\"\n",
        "    tokenized = word_tokenize(sentence.lower())\n",
        "        \n",
        "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
        "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
        "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
        "    \n",
        "    return numberized\n",
        "\n",
        "def preprocess_one_hot(sentence):\n",
        "    \"\"\"\n",
        "    Given a sentence, in the form of a string, this function will preprocess it\n",
        "    into a numpy array of one-hot vectors.\n",
        "    \"\"\"\n",
        "    numberized = preprocess_numberize(sentence)\n",
        "    \n",
        "    # Represent each word as it's one-hot embedding\n",
        "    one_hot_embedded = one_hot_embeddings[numberized]\n",
        "    \n",
        "    return one_hot_embedded\n",
        "\n",
        "def preprocess_word2vec(sentence):\n",
        "    \"\"\"\n",
        "    Given a sentence, in the form of a string, this function will preprocess it\n",
        "    into a numpy array of word2vec embeddings.\n",
        "    \"\"\"\n",
        "    numberized = preprocess_numberize(sentence)\n",
        "    \n",
        "    # Represent each word as it's one-hot embedding\n",
        "    w2v_embedded = w2v_embeddings[numberized]\n",
        "    \n",
        "    return w2v_embedded\n",
        "\n",
        "def compute_bleu(reference_sentence, predicted_sentence):\n",
        "    \"\"\"\n",
        "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
        "    \"\"\"\n",
        "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
        "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
        "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n",
        "\n",
        "%matplotlib inline\n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
        "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
        "\n",
        "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
        "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score distnace between \n",
            "  \"A very clean and well decorated empty bathroom\" \n",
            "and\n",
            "  \"A very clean and well decorated empty bathroom\" \n",
            "is: 1.0\n",
            "\n",
            "\n",
            "BLEU score distnace between \n",
            "  \"A very clean and well decorated empty bathroom\" \n",
            "and\n",
            "  \"A few people sit on a dim transportation system. \" \n",
            "is: 0.1933853138176172\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZD-r_MckSLA"
      },
      "source": [
        "#Part 1: Encoder-Decoder Language Translation with Teacher-Forcing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSOKOJModkdN"
      },
      "source": [
        "## 1.1 Building a Language Decoder\n",
        "\n",
        "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
        "\n",
        "Remember to use LSTM hidden units!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "w7g1DsQcdkdO"
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = 300\n",
        "        wordEncodingSize = 2000\n",
        "        \n",
        "        # Your code goes here (~4 lines or less)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return # Your code goes here (1 line)\n",
        "\n",
        "\n",
        "    def init_cell(self):\n",
        "        return # Your code goes here (1 line)\n",
        "\n",
        "    def forward(self, input_sentence, hidden, cell):\n",
        "      # Your code goes here (~4 lines or less)\n",
        "      \n",
        "      return output, hidden, cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJVTRdUkdkdR"
      },
      "source": [
        "## 1.2.  Building Language Encoder\n",
        "\n",
        "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kccifunUdkdR"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    # Your code goes here\n",
        "    def __init__(self):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = 300\n",
        "        wordEncodingSize = 2000\n",
        "        \n",
        "        # Your code goes here (~3 lines)\n",
        "       \n",
        "    def init_hidden(self):\n",
        "        return # Your code goes here (1 line)\n",
        "\n",
        "    def init_cell(self):\n",
        "        return # Your code goes here (1 line)\n",
        "\n",
        "    def forward(self, input_sentence, hidden, cell):\n",
        "        # Your code goes here (~2 lines of code)\n",
        "        \n",
        "        return output, hidden , cell\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljBX3m0tdkdT"
      },
      "source": [
        "## 1.3. Connecting Encoder to Decoder and Train End-to-End and Train with Teacher Forcing\n",
        "\n",
        "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
        "\n",
        "For the purposes of Part 1, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. This will be different for Part 2 and 3 where we will extend this punction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kN5nFk_ndkdU"
      },
      "source": [
        "def train(input_sentence, output_sentence, encoder,\n",
        "          decoder, encoder_optimizer,\n",
        "          decoder_optimizer, \n",
        "          criterion, \n",
        "          teacher_forcing_ratio = 1,\n",
        "          decoderType = \"LSTM\",\n",
        "          embeddings = one_hot_embeddings): \n",
        "    \"\"\"\n",
        "    Given a single training sample, go through a single step of training.\n",
        "    \"\"\"\n",
        "    use_teacher_forcing = True if np.random.rand() < teacher_forcing_ratio else False\n",
        "    # Your code goes here\n",
        "\n",
        "    if decoderType == \"LSTM\": \n",
        "        if use_teacher_forcing: \n",
        "            # Your code goes here\n",
        "        elif not use_teacher_forcing:\n",
        "            # Your code goes here\n",
        "\n",
        "    if decoderType == \"AttentionLSTM\": \n",
        "        if use_teacher_forcing: \n",
        "            # Your code goes here\n",
        "        elif not use_teacher_forcing:\n",
        "            # Your code goes here\n",
        "\n",
        "    if decoderType == \"Transformer\":\n",
        "        # Your code goes here (you can assume transformer uses teacher_forcing_ratio = 1)\n",
        "\n",
        "    # Your code goes here\n",
        "    return final_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaEuQm7KwHBJ"
      },
      "source": [
        "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
        "encoder = EncoderLSTM()\n",
        "decoder = DecoderLSTM()\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "\n",
        "print(\"Start training end-to-end network ......\")\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss=[]\n",
        "    count=0\n",
        "    for id, sentence in enumerate(filtered_sentences):\n",
        "        target_variable = piglatin_filtered_sentences[id]\n",
        "        loss = train(sentence, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"LSTM\")\n",
        "        \n",
        "        count = count+1\n",
        "        if count%500==0:\n",
        "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
        "        epoch_loss.append(loss)\n",
        "        \n",
        "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-KzySeXdkdP"
      },
      "source": [
        "## 1.4. Building Language Decoder MAP Inference\n",
        "\n",
        "We now define a method to perform inference with our decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Fq1fkD-wdkdP"
      },
      "source": [
        "def inference(sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
        "    input_tensor = torch.Tensor(preprocess_one_hot(sentence))\n",
        "    input_length = input_tensor.shape[0]\n",
        "\n",
        "    # Initialize encoder & decoder \n",
        "\n",
        "    for ei in range(1,input_length):\n",
        "        # Iteratively run the encoder \n",
        "        # 1. Get the current word index\n",
        "        # 2. Convert to a 1-hot encoding\n",
        "        # 3. Run one step of the encoder\n",
        "        # 4. Save the encoder hidden states for future processing\n",
        "\n",
        "    # Set the initial hidden and cell state of the RNN decoder to the last \n",
        "    # hidden and cell state of the encoder\n",
        "\n",
        "    # Start the decoding with <SOS> token\n",
        "\n",
        "    # Iterate up to the max_length of output\n",
        "    for i in range(max_length):\n",
        "        if decoderType == \"LSTM\": \n",
        "            # Run the simple decoder \n",
        "\n",
        "        if decoderType == \"AttentionLSTM\":\n",
        "            # Run the attention decoder (this will be done in Part 2)\n",
        "\n",
        "        if decoderType == \"Transformer\":\n",
        "            # Run the transformer decoder (this will be done in Part 3)\n",
        "\n",
        "    return word_list, decoder_attentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E3scOrkxtHV"
      },
      "source": [
        "# Lets test it \n",
        "sentence = \"A very clean and well decorated empty bathroom\" \n",
        "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
        "output_sentence, _ = inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
        "\n",
        "print(\"English: \" + sentence)\n",
        "print(\"Pig Latin: \" + output_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaZkwa6EdkdQ"
      },
      "source": [
        "## 1.5. Building Language Decoder Sampling Inference\n",
        "\n",
        "We now modify the inference method to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
        "\n",
        "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "chHsbrX8dkdQ"
      },
      "source": [
        "def sampling_inference(sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
        "    input_tensor = torch.Tensor(preprocess_one_hot(sentence))\n",
        "    input_length = input_tensor.shape[0]\n",
        "\n",
        "    # Initialize encoder & decoder \n",
        "\n",
        "    for ei in range(1,input_length):\n",
        "        # Iteratively run the encoder \n",
        "        # 1. Get the current word index\n",
        "        # 2. Convert to a 1-hot encoding\n",
        "        # 3. Run one step of the encoder\n",
        "        # 4. Save the encoder hidden states for future processing\n",
        "\n",
        "    # Set the initial hidden and cell state of the RNN decoder to the last \n",
        "    # hidden and cell state of the encoder\n",
        "\n",
        "    # Start the decoding with <SOS> token\n",
        "\n",
        "    # Iterate up to the max_length of output\n",
        "    for i in range(max_length):\n",
        "        if decoderType == \"LSTM\": \n",
        "            # Run the simple decoder \n",
        "\n",
        "        if decoderType == \"AttentionLSTM\":\n",
        "            # Run the attention decoder (this will be done in Part 2)\n",
        "\n",
        "        if decoderType == \"Transformer\":\n",
        "            # Run the transformer decoder (this will be done in Part 3)\n",
        "\n",
        "\n",
        "    return word_list, decoder_attentions\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6jeTIFOyfa7"
      },
      "source": [
        "# Lets test it \n",
        "sentence = \"A very clean and well decorated empty bathroom\" \n",
        "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
        "\n",
        "print(\"English: \" + sentence)\n",
        "\n",
        "for i in range(5):\n",
        "    output_sentence, _ = sampling_inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
        "    print(\"Pig Latin: \" + output_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe3Aq_-VdkdV"
      },
      "source": [
        "## 1.6. Testing \n",
        "\n",
        "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rcxSh_RWdkdR"
      },
      "source": [
        "# Perform inference for all validation sequences and report the average BLEU score\n",
        "avg_score=[]\n",
        "\n",
        "# iterate over the validation set \n",
        "for idx, input_sentence in enumerate(val_sentences): \n",
        "    # output_sentence, _ = inference(...)\n",
        "    # target_sentence = ... \n",
        "    # score = compute_blue(...)\n",
        "    avg_score.append(score)\n",
        "    if idx < 10 :\n",
        "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
        "\n",
        "final_score = np.sum(avg_score)/len(val_sentences)\n",
        "print(\"Average BLUE score : %f\" % (final_score)) \n",
        "\n",
        "\n",
        "# EXPECTED < Average BLUE score (ArgMAX inference): 0.464803 > \n",
        "# EXPECTED < Average BLUE score (sampling inference): 0.477803 > "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL3LKIZ7dkdQ"
      },
      "source": [
        "## 1.7. Experiment with Teacher Forcing\n",
        "\n",
        "Redo steps 1.3 and 1.6 with teacher_forcing_ratio = 0.9 and 0.8. Comment on the results, speed of convergence and the quality of results. Note that in most real scenarious the teacher forcing is actually annealed; starting with teacher forcing = 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DX-D_PI7dkdV"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3wDN2Z1dkdX"
      },
      "source": [
        "## 1.8. Encoding as Generic Feature Representation\n",
        "\n",
        "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
        "\n",
        "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IxFmDA_YdkdY"
      },
      "source": [
        "def final_encoder_hidden(sentence):\n",
        "    # Your code goes here\n",
        "\n",
        "# Now run all training data and validation data to store hidden states\n",
        "    # Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RpZtVzHpdkdY"
      },
      "source": [
        "# Now get nearest neighbors and print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFpfb1oi0efe"
      },
      "source": [
        "# Part 2: Attention LSTM Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx2bELJh03hq"
      },
      "source": [
        "## 2.1. Implementing Additive Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2adPSOL09Hj"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x 1 x seq_len)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # Your code goes here\n",
        "        # ------------\n",
        "        # batch_size = 1\n",
        "        # expanded_queries = ...\n",
        "        # concat_inputs = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "\n",
        "        return context, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOPm_dvX1TrZ"
      },
      "source": [
        "## 2.2. Attention Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY7vHNT_1oug"
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "\n",
        "        self.hidden_dim = 300\n",
        "        wordEncodingSize = 2000\n",
        "        self.dropout_p = 0.1\n",
        "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.hidden_dim*2, self.hidden_dim)\n",
        "        self.attention = AdditiveAttention(hidden_size=self.hidden_dim)\n",
        "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
        "        self.hidden = self.init_hidden()\n",
        "        self.cell = self.init_cell()\n",
        "\n",
        "    def init_hidden(self):    \n",
        "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
        "\n",
        "    def init_cell(self):\n",
        "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
        "\n",
        "    def forward(self, input_sentence, hidden, cell, encoder_annotations):\n",
        "        embed = self.dropout(self.linear_input(input_sentence.view(1,-1)))\n",
        "\n",
        "        # ------------\n",
        "        # Your code goes here\n",
        "        # ------------\n",
        "        # embed_current = ...\n",
        "        # context, attention_weights = ...\n",
        "        # embed_and_context = ...\n",
        "        \n",
        "        return output, hidden, cell, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7mTRflmQQu"
      },
      "source": [
        "## 2.3. Training Attention Decoder\n",
        "\n",
        "Note that you will need to modify the train() procedure for Part 1 to handles the AttentionLSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEYM3U0VmdHp"
      },
      "source": [
        "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
        "encoder = EncoderLSTM()\n",
        "decoder = AttentionDecoder()\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "\n",
        "print(\"Start training end to end network ......\")\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss=[]\n",
        "    count=0\n",
        "    for id, sentence in enumerate(filtered_sentences):\n",
        "        target_variable = piglatin_filtered_sentences[id]\n",
        "        loss = train(sentence, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"AttentionLSTM\")\n",
        "        count = count+1\n",
        "        if count%500==0:\n",
        "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
        "        epoch_loss.append(loss)\n",
        "        \n",
        "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFTuFon2m7f9"
      },
      "source": [
        "## 2.4. Testing Attention Decoder\n",
        "Note that you will need to modify the inference() procedure for Part 1 to handle Attention LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx3flyVdnGCg"
      },
      "source": [
        "# Perform inference for all validation sequences and report the average BLEU score\n",
        "avg_score=[]\n",
        "\n",
        "# iterate over the validation set \n",
        "for idx, input_sentence in enumerate(val_sentences): \n",
        "    # output_sentence, _ = inference(...)\n",
        "    # target_sentence = ... \n",
        "    # score = compute_blue(...)\n",
        "    avg_score.append(score)\n",
        "    if idx < 10 :\n",
        "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
        "\n",
        "final_score = np.sum(avg_score)/len(val_sentences)\n",
        "print(\"Average BLUE score : %f\" % (final_score)) \n",
        "\n",
        "# EXPECTED < Average BLUE score (ArgMAX inference): 0.739589 >"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvKl0zhhngVg"
      },
      "source": [
        "## 2.5. Visualize Attention for Attention Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsAfscbVnhF6"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XABkHOBJns76"
      },
      "source": [
        "# Part 3: Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFb5tCzOpDmB"
      },
      "source": [
        "## 3.1 Implement Scaled Dot Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDgSOIvdpCuR"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # Your code goes here\n",
        "        # ------------\n",
        "        # batch_size = 1\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "\n",
        "        return context, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4gpSwDopbHZ"
      },
      "source": [
        "## 3.2. Implement Causal Scaled Dot Attention\n",
        "\n",
        "The implementation should be nearly identical to the one above, but with mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFnGpHklpbpQ"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7).cuda()\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        NOTES:\n",
        "            batch_size = 1\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "                In training k = maxSequenceLength or length of the GT ourput sequence\n",
        "                In testing k = length of currently decoded sub-sequence\n",
        "            keys: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # Your code goes here\n",
        "        # ------------\n",
        "        # batch_size = 1\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # mask = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "\n",
        "        return context, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsLrprNLqQ5x"
      },
      "source": [
        "## 3.3. Implement Transformer Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjiwfUHXqRXI"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.hidden_dim = 300\n",
        "        wordEncodingSize = 2000\n",
        "        self.dropout_p = 0.1\n",
        "        self.num_layers = 3\n",
        "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=self.hidden_dim, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=self.hidden_dim, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
        "\n",
        "    def forward(self, input_sentence, hidden, cell, annotations):\n",
        "        embed = self.dropout(self.linear_input(input_sentence)).unsqueeze(0)\n",
        "        \n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        batch_size, seq_len, hidden_size = contexts.size()\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # ------------\n",
        "            # Your code goes here\n",
        "            # ------------\n",
        "            \n",
        "            \n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)            \n",
        "        \n",
        "        output = self.linear(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, encoder_attention_weights, self_attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENolyKTWrAWq"
      },
      "source": [
        "## 3.4. Training Transformer Decoder\n",
        "\n",
        "Note that you will need to modify the train() procedure for Part 1 to handle the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iuUnzkZrA44"
      },
      "source": [
        "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
        "encoder = EncoderLSTM()\n",
        "decoder = TransformerDecoder()\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "\n",
        "print(\"Start training end to end network ......\")\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss=[]\n",
        "    count=0\n",
        "    for id, sentence in enumerate(filtered_sentences):\n",
        "        target_variable = piglatin_filtered_sentences[id]\n",
        "        loss = train(sentence, target_variable, encoder, decoder,encoder_optimizer,decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"Transformer\")\n",
        "        count = count+1\n",
        "        if count%500==0:\n",
        "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
        "        epoch_loss.append(loss)\n",
        "        \n",
        "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOF6M3tWrbOS"
      },
      "source": [
        "## 3.5. Testing Transformer Decoder\n",
        "Note that you will need to modify the inference() procedure for Part 1 to handle Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2BlDGfcroOA"
      },
      "source": [
        "# Perform inference for all validation sequences and report the average BLEU score\n",
        "avg_score=[]\n",
        "\n",
        "# iterate over the validation set \n",
        "for idx, input_sentence in enumerate(val_sentences): \n",
        "    # output_sentence, _ = inference(...)\n",
        "    # target_sentence = ... \n",
        "    # score = compute_blue(...)\n",
        "    avg_score.append(score)\n",
        "    if idx < 10 :\n",
        "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
        "\n",
        "final_score = np.sum(avg_score)/len(val_sentences)\n",
        "print(\"Average BLUE score : %f\" % (final_score)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrOGW2NrzPG"
      },
      "source": [
        "## 3.6 Visualizing Attention for Transformer Decoder\n",
        "\n",
        "Note that since we have multiple attention layers, there will be one attention to be visualized per layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jng5Ksadkdp"
      },
      "source": [
        "# 4. Effectiveness of word2vec\n",
        "\n",
        "As an option, you may repeat one of the models above by modifying the code to use word2vec embedding for the input English sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RWVVvgyudkdq"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}